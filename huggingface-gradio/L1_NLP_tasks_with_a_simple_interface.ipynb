{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88535ead",
   "metadata": {},
   "source": [
    "# L1: NLP tasks with a simple interface ðŸ—žï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa43ba",
   "metadata": {},
   "source": [
    "Load your HF API key and relevant Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2698081-4deb-436a-a821-8ea48bdd6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from IPython.display import Image, display, HTML\n",
    "from PIL import Image\n",
    "import base64 \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a106ab02-f248-4c03-9dd8-b1991db7f778",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "shleifer/distilbart-cnn-12-6 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 261\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/shleifer/distilbart-cnn-12-6/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\huggingface_hub\\file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\huggingface_hub\\file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1532\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1533\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1534\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1539\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1540\u001b[0m )\n\u001b[1;32m-> 1541\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:293\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    285\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m     )\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-65216b30-6bb61f952018698c03be7601;ee685bec-dfac-4525-8d15-03633a65492f)\n\nRepository Not Found for url: https://huggingface.co/shleifer/distilbart-cnn-12-6/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 4\u001b[0m get_completion \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshleifer/distilbart-cnn-12-6\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\transformers\\pipelines\\__init__.py:705\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 705\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m    706\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m    708\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:983\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    982\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 983\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    984\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    985\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\transformers\\configuration_utils.py:617\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    619\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\transformers\\configuration_utils.py:672\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 672\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\transformers\\utils\\hub.py:433\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m     )\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    444\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: shleifer/distilbart-cnn-12-6 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "# Helper function\n",
    "import requests, json\n",
    "from transformers import pipeline\n",
    "get_completion = pipeline(\"summarization\", model=\"shleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "#Summarization endpoint\n",
    "# def get_completion2(inputs, parameters=None,ENDPOINT_URL=os.environ['HF_API_SUMMARY_BASE']): \n",
    "#     headers = {\n",
    "#       \"Authorization\": f\"Bearer {hf_api_key}\",\n",
    "#       \"Content-Type\": \"application/json\"\n",
    "#     }\n",
    "#     data = { \"inputs\": inputs }\n",
    "#     if parameters is not None:\n",
    "#         data.update({\"parameters\": parameters})\n",
    "#     response = requests.request(\"POST\",\n",
    "#                                 ENDPOINT_URL, headers=headers,\n",
    "#                                 data=json.dumps(data)\n",
    "#                                )\n",
    "#     return json.loads(response.content.decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a06f9",
   "metadata": {},
   "source": [
    "## Building a text summarization app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4794502",
   "metadata": {},
   "source": [
    "Here we are using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for the `shleifer/distilbart-cnn-12-6`, a 306M parameter distilled model from `facebook/bart-large-cnn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01892d1a",
   "metadata": {},
   "source": [
    "### How about running it locally?\n",
    "The code would look very similar if you were running it locally instead of from an API. The same is true for all the models in the rest of the course, make sure to check the [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) documentation page\n",
    "\n",
    "```py\n",
    "from transformers import pipeline\n",
    "\n",
    "get_completion = pipeline(\"summarization\", model=\"shleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0fc58-91d6-48f2-a014-052192586be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ('''The tower is 324 metres (1,063 ft) tall, about the same height\n",
    "        as an 81-storey building, and the tallest structure in Paris. \n",
    "        Its base is square, measuring 125 metres (410 ft) on each side. \n",
    "        During its construction, the Eiffel Tower surpassed the Washington \n",
    "        Monument to become the tallest man-made structure in the world,\n",
    "        a title it held for 41 years until the Chrysler Building\n",
    "        in New York City was finished in 1930. It was the first structure \n",
    "        to reach a height of 300 metres. Due to the addition of a broadcasting \n",
    "        aerial at the top of the tower in 1957, it is now taller than the \n",
    "        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the \n",
    "        Eiffel Tower is the second tallest free-standing structure in France \n",
    "        after the Millau Viaduct.''')\n",
    "\n",
    "get_completion(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144593f",
   "metadata": {},
   "source": [
    "### Getting started with Gradio `gr.Interface` \n",
    "\n",
    "#### How about running it locally?\n",
    "The code would look very similar if you were running it locally.  Simply remove all the paramters in the launch method\n",
    "\n",
    "```py\n",
    "demo.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb11460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=summarize, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b310770",
   "metadata": {},
   "source": [
    "You can add `demo.launch(share=True)` to create a public link to share with your team or friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60684b55-c7ae-4c9e-88ea-bbc2e702ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def summarize(input):\n",
    "    output = get_completion(input)\n",
    "    return output[0]['summary_text']\n",
    "    \n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=summarize, \n",
    "                    inputs=[gr.Textbox(label=\"Text to summarize\", lines=6)],\n",
    "                    outputs=[gr.Textbox(label=\"Result\", lines=3)],\n",
    "                    title=\"Text summarization with distilbart-cnn\",\n",
    "                    description=\"Summarize any text using the `shleifer/distilbart-cnn-12-6` model under the hood!\"\n",
    "                   )\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b300d17",
   "metadata": {},
   "source": [
    "## Building a Named Entity Recognition app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1043f",
   "metadata": {},
   "source": [
    "We are using this [Inference Endpoint](https://huggingface.co/inference-endpoints) for `dslim/bert-base-NER`, a 108M parameter fine-tuned BART model on the NER task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f663dcbb",
   "metadata": {},
   "source": [
    "### How about running it locally?\n",
    "\n",
    "```py\n",
    "from transformers import pipeline\n",
    "\n",
    "get_completion = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input)\n",
    "    return {\"text\": input, \"entities\": output}\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4a922-b300-4dbc-8768-955b6a18dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = os.environ['HF_API_NER_BASE'] #NER endpoint\n",
    "text = \"My name is Andrew, I'm building DeepLearningAI and I live in California\"\n",
    "get_completion(text, parameters=None, ENDPOINT_URL= API_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c21254-128d-446c-b6dd-e30af26d436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(input):\n",
    "    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    return {\"text\": input, \"entities\": output}\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "                    title=\"NER with dslim/bert-base-NER\",\n",
    "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    #Here we introduce a new tag, examples, easy to use examples for your application\n",
    "                    examples=[\"My name is Andrew and I live in California\", \"My name is Poli and work at HuggingFace\"])\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f16ad4",
   "metadata": {},
   "source": [
    "### Adding a helper function to merge tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc278e9-87b4-420b-89e9-7120dc4be754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(tokens):\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
    "            # If current token continues the entity of the last one, merge them\n",
    "            last_token = merged_tokens[-1]\n",
    "            last_token['word'] += token['word'].replace('##', '')\n",
    "            last_token['end'] = token['end']\n",
    "            last_token['score'] = (last_token['score'] + token['score']) / 2\n",
    "        else:\n",
    "            # Otherwise, add the token to the list\n",
    "            merged_tokens.append(token)\n",
    "\n",
    "    return merged_tokens\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)\n",
    "    merged_tokens = merge_tokens(output)\n",
    "    return {\"text\": input, \"entities\": merged_tokens}\n",
    "\n",
    "gr.close_all()\n",
    "demo = gr.Interface(fn=ner,\n",
    "                    inputs=[gr.Textbox(label=\"Text to find entities\", lines=2)],\n",
    "                    outputs=[gr.HighlightedText(label=\"Text with entities\")],\n",
    "                    title=\"NER with dslim/bert-base-NER\",\n",
    "                    description=\"Find entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"My name is Andrew, I'm building DeeplearningAI and I live in California\", \"My name is Poli, I live in Vienna and work at HuggingFace\"])\n",
    "\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cccdb9b-0c3a-406e-95bc-106705aeb010",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c428938-aec5-46f6-9315-af93a7a8abb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47b403-f187-4d7f-929e-a1c09012b741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33eac55-3520-443a-a7fd-a2a6c8768ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d1a9d-9f3a-4f38-bdbc-08b97d17a72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468c2f8-5123-4121-962c-326befeb7789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda551f-ca2e-4a8c-8fc0-f921ba1eb172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af69688-df59-45a6-be9f-1e66cae7ac1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b808da8-a99e-4fd0-b984-32640d90c13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc373ed3-ce8d-402e-bbda-837e9478c2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eba1e9-e994-4480-9076-99a7b71d3211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189844a4-1fcc-4ccc-bcff-9c9d3ec6759c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce126b-c54e-4fec-8d4b-462317149f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833b2ac-f605-4566-b84a-c0c2bc529bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1d07f-ac8b-4f38-9c6c-ecdd5942ffb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c6dd3-bc15-4139-bee8-d7291c20a1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddfa37-da43-40be-a3da-05aa3d2ee9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50dafb5-50bf-4b8f-9eec-84682ae339e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13755d46-db04-435f-bbe8-c6b6df9f46fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea1054-75c1-42c5-8fe3-1f5a1eb9b8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00aba6-d0e7-49be-816c-0fa4670737fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
